{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10968257,"sourceType":"datasetVersion","datasetId":6824380}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom tqdm import tqdm\n\n# Directory containing the anthem files\nanthems_dir = \"/kaggle/input/national-anthem-sound-dataset\"\nresults = []\n\n# Process each anthem file\nfor filename in tqdm(os.listdir(anthems_dir)):\n    if filename.endswith(\".mp3\"):\n        filepath = os.path.join(anthems_dir, filename)\n        try:\n            # Load the audio file\n            y, sr = librosa.load(filepath, sr=None)\n            \n            # Extract country name from filename (assuming format like \"USA_anthem.mp3\")\n            country = filename.split('_')[0]\n            \n            # Basic audio properties\n            duration = librosa.get_duration(y=y, sr=sr)\n            \n            # Extract features\n            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n            chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n            zero_crossing_rate = librosa.feature.zero_crossing_rate(y=y)\n            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n            \n            # Calculate statistics\n            chroma_mean = np.mean(chroma)\n            centroid_mean = np.mean(spectral_centroid)\n            rolloff_mean = np.mean(spectral_rolloff)\n            zcr_mean = np.mean(zero_crossing_rate)\n            mfcc_means = np.mean(mfccs, axis=1)\n            \n            # Calculate rhythm strength\n            onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n            rhythm_strength = np.mean(onset_env)\n            \n            # Calculate harmonic-percussive ratio\n            y_harmonic, y_percussive = librosa.effects.hpss(y)\n            harmonic_percussive_ratio = np.sum(np.abs(y_harmonic)) / np.sum(np.abs(y_percussive))\n            \n            # Store results\n            results.append({\n                'country': country,\n                'duration': duration,\n                'tempo': tempo,\n                'chroma_mean': chroma_mean,\n                'centroid_mean': centroid_mean,\n                'rolloff_mean': rolloff_mean,\n                'zcr_mean': zcr_mean,\n                'rhythm_strength': rhythm_strength,\n                'harmonic_percussive_ratio': harmonic_percussive_ratio,\n                'mfcc1': mfcc_means[0],\n                'mfcc2': mfcc_means[1],\n                'mfcc3': mfcc_means[2],\n                'mfcc4': mfcc_means[3],\n                'mfcc5': mfcc_means[4]\n            })\n        except Exception as e:\n            print(f\"Error processing {filename}: {e}\")\n\n# Convert to DataFrame\ndf = pd.DataFrame(results)\ndf.to_csv(\"anthem_analysis.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T08:39:05.186293Z","iopub.execute_input":"2025-03-09T08:39:05.186551Z","iopub.status.idle":"2025-03-09T09:02:22.268341Z","shell.execute_reply.started":"2025-03-09T08:39:05.186524Z","shell.execute_reply":"2025-03-09T09:02:22.266898Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [23:13<00:00, 13.93s/it]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Continue from previous script\n# Basic statistics\nprint(\"Summary statistics:\")\nprint(df.describe())\n\n# Correlations\nprint(\"\\nCorrelation matrix:\")\ncorrelation_matrix = df.select_dtypes(include=[np.number]).corr()\nprint(correlation_matrix)\n\n# Clustering\nfeatures = ['tempo', 'centroid_mean', 'rolloff_mean', 'zcr_mean', \n            'rhythm_strength', 'harmonic_percussive_ratio',\n            'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5']\nX = df[features].values\n\n# Normalize features\nfrom sklearn.preprocessing import StandardScaler\nX_scaled = StandardScaler().fit_transform(X)\n\n# PCA for visualization\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Determine optimal number of clusters\nfrom sklearn.metrics import silhouette_score\nsilhouette_scores = []\nfor k in range(2, 10):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    cluster_labels = kmeans.fit_predict(X_scaled)\n    silhouette_scores.append(silhouette_score(X_scaled, cluster_labels))\n\noptimal_k = silhouette_scores.index(max(silhouette_scores)) + 2\nprint(f\"\\nOptimal number of clusters: {optimal_k}\")\n\n# Apply K-means with optimal number of clusters\nkmeans = KMeans(n_clusters=optimal_k, random_state=42)\ndf['cluster'] = kmeans.fit_predict(X_scaled)\n\n# Plot the clusters\nplt.figure(figsize=(12, 8))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['cluster'], cmap='viridis', alpha=0.6)\nplt.title('National Anthem Clusters based on Audio Features')\nplt.xlabel('PCA Component 1')\nplt.ylabel('PCA Component 2')\n\n# Add country labels\nfor i, country in enumerate(df['country']):\n    plt.annotate(country, (X_pca[i, 0], X_pca[i, 1]), fontsize=8)\n\nplt.colorbar(scatter, label='Cluster')\nplt.savefig('anthem_clusters.png')\nplt.close()\n\n# Analyze characteristics of each cluster\nprint(\"\\nCluster analysis:\")\nfor cluster in range(optimal_k):\n    cluster_anthems = df[df['cluster'] == cluster]\n    print(f\"\\nCluster {cluster} - {len(cluster_anthems)} anthems\")\n    print(\"Countries:\", \", \".join(cluster_anthems['country'].tolist()))\n    print(\"Average characteristics:\")\n    for feature in features:\n        # Check if the value is a scalar or array\n        mean_value = cluster_anthems[feature].mean()\n        if isinstance(mean_value, np.ndarray):\n            # For array values, print first few elements\n            print(f\"  {feature}: {mean_value[:3]}...\")\n        else:\n            # For scalar values, format with 2 decimal places\n            print(f\"  {feature}: {mean_value:.2f}\")\n\n# Regional analysis\n# Assuming country names can be mapped to continents (you'd need to add this mapping)\ncontinent_map = {\n    # Add your mappings here, e.g., 'USA': 'North America', 'France': 'Europe', etc.\n}\n\n# Add continent column if you have the mapping\nif continent_map:\n    df['continent'] = df['country'].map(continent_map)\n    \n    # Plot features by continent\n    for feature in ['tempo', 'duration', 'harmonic_percussive_ratio']:\n        plt.figure(figsize=(10, 6))\n        df.boxplot(column=feature, by='continent')\n        plt.title(f'{feature} by Continent')\n        plt.suptitle('')\n        plt.savefig(f'{feature}_by_continent.png')\n        plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T09:31:35.621297Z","iopub.execute_input":"2025-03-09T09:31:35.621979Z","iopub.status.idle":"2025-03-09T09:31:36.355842Z","shell.execute_reply.started":"2025-03-09T09:31:35.621870Z","shell.execute_reply":"2025-03-09T09:31:36.353901Z"}},"outputs":[{"name":"stdout","text":"Summary statistics:\n         duration  chroma_mean  centroid_mean  rolloff_mean    zcr_mean  \\\ncount  100.000000   100.000000     100.000000    100.000000  100.000000   \nmean    81.779284     0.351368    1717.036340   3294.458904    0.046718   \nstd     34.135738     0.028317     434.120002    987.987502    0.010024   \nmin     29.648980     0.280847     929.119155   1616.349821    0.027781   \n25%     57.593469     0.330477    1406.361731   2584.570035    0.039278   \n50%     74.540408     0.352312    1647.174459   3117.416224    0.044554   \n75%     99.617959     0.368098    2007.752263   3948.828069    0.052937   \nmax    213.342041     0.416764    2749.653069   5649.751232    0.078394   \n\n       rhythm_strength  harmonic_percussive_ratio       mfcc1       mfcc2  \\\ncount       100.000000                 100.000000  100.000000  100.000000   \nmean          0.842743                   3.922095 -248.512924  183.486938   \nstd           0.095282                   1.232348   51.580925   20.702450   \nmin           0.591736                   2.485544 -385.288940  133.888962   \n25%           0.783304                   3.013103 -283.460953  169.245865   \n50%           0.849923                   3.641528 -244.729774  184.998169   \n75%           0.908266                   4.399699 -211.770733  197.710670   \nmax           1.038712                   7.646672 -117.222282  234.454391   \n\n            mfcc3       mfcc4       mfcc5     cluster  \ncount  100.000000  100.000000  100.000000  100.000000  \nmean   -32.742081   20.339226   -6.106187    0.640000  \nstd     13.503780   13.997851   10.540615    0.482418  \nmin    -75.035912  -19.122534  -35.590294    0.000000  \n25%    -39.828829    8.695838  -11.868245    0.000000  \n50%    -32.352205   21.188430   -5.620976    1.000000  \n75%    -25.875536   28.810477    1.727292    1.000000  \nmax      4.118566   54.331043   18.696741    1.000000  \n\nCorrelation matrix:\n                           duration  chroma_mean  centroid_mean  rolloff_mean  \\\nduration                   1.000000    -0.158569      -0.024682     -0.020258   \nchroma_mean               -0.158569     1.000000       0.184872      0.231104   \ncentroid_mean             -0.024682     0.184872       1.000000      0.989035   \nrolloff_mean              -0.020258     0.231104       0.989035      1.000000   \nzcr_mean                  -0.041181     0.016843       0.879288      0.820811   \nrhythm_strength            0.059419     0.175147       0.614723      0.606554   \nharmonic_percussive_ratio -0.005239    -0.528138      -0.411016     -0.449265   \nmfcc1                      0.007152     0.223605       0.632659      0.610294   \nmfcc2                     -0.015195    -0.156015      -0.759004     -0.759184   \nmfcc3                      0.215911    -0.075712      -0.701816     -0.641231   \nmfcc4                     -0.125173     0.314908       0.828230      0.858668   \nmfcc5                      0.063168    -0.005383      -0.852835     -0.833437   \ncluster                    0.063209    -0.130659      -0.820833     -0.834249   \n\n                           zcr_mean  rhythm_strength  \\\nduration                  -0.041181         0.059419   \nchroma_mean                0.016843         0.175147   \ncentroid_mean              0.879288         0.614723   \nrolloff_mean               0.820811         0.606554   \nzcr_mean                   1.000000         0.448610   \nrhythm_strength            0.448610         1.000000   \nharmonic_percussive_ratio -0.144087        -0.711294   \nmfcc1                      0.515572         0.634365   \nmfcc2                     -0.642474        -0.267076   \nmfcc3                     -0.789126        -0.502318   \nmfcc4                      0.591529         0.656326   \nmfcc5                     -0.786999        -0.468251   \ncluster                   -0.685386        -0.467220   \n\n                           harmonic_percussive_ratio     mfcc1     mfcc2  \\\nduration                                   -0.005239  0.007152 -0.015195   \nchroma_mean                                -0.528138  0.223605 -0.156015   \ncentroid_mean                              -0.411016  0.632659 -0.759004   \nrolloff_mean                               -0.449265  0.610294 -0.759184   \nzcr_mean                                   -0.144087  0.515572 -0.642474   \nrhythm_strength                            -0.711294  0.634365 -0.267076   \nharmonic_percussive_ratio                   1.000000 -0.421113  0.173268   \nmfcc1                                      -0.421113  1.000000 -0.266868   \nmfcc2                                       0.173268 -0.266868  1.000000   \nmfcc3                                       0.197863 -0.596435  0.402388   \nmfcc4                                      -0.567158  0.606975 -0.578972   \nmfcc5                                       0.195583 -0.550749  0.621837   \ncluster                                     0.346789 -0.539046  0.635489   \n\n                              mfcc3     mfcc4     mfcc5   cluster  \nduration                   0.215911 -0.125173  0.063168  0.063209  \nchroma_mean               -0.075712  0.314908 -0.005383 -0.130659  \ncentroid_mean             -0.701816  0.828230 -0.852835 -0.820833  \nrolloff_mean              -0.641231  0.858668 -0.833437 -0.834249  \nzcr_mean                  -0.789126  0.591529 -0.786999 -0.685386  \nrhythm_strength           -0.502318  0.656326 -0.468251 -0.467220  \nharmonic_percussive_ratio  0.197863 -0.567158  0.195583  0.346789  \nmfcc1                     -0.596435  0.606975 -0.550749 -0.539046  \nmfcc2                      0.402388 -0.578972  0.621837  0.635489  \nmfcc3                      1.000000 -0.608417  0.693298  0.562619  \nmfcc4                     -0.608417  1.000000 -0.754635 -0.716653  \nmfcc5                      0.693298 -0.754635  1.000000  0.706877  \ncluster                    0.562619 -0.716653  0.706877  1.000000  \n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nOptimal number of clusters: 2\n\nCluster analysis:\n\nCluster 0 - 36 anthems\nCountries: gw.mp3, mc.mp3, kp.mp3, ke.mp3, bj.mp3, bz.mp3, gg.mp3, ki.mp3, bb.mp3, bd.mp3, es.mp3, fk.mp3, md.mp3, gu.mp3, et.mp3, bh.mp3, as.mp3, bs.mp3, eu.mp3, kw.mp3, be.mp3, ao.mp3, bl.mp3, fr.mp3, by.mp3, bn.mp3, jo.mp3, ie.mp3, bw.mp3, id.mp3, at.mp3, ir.mp3, bo.mp3, kg.mp3, er.mp3, gh.mp3\nAverage characteristics:\n  tempo: [103.80074742]...\n  centroid_mean: 2189.77\n  rolloff_mean: 4387.92\n  zcr_mean: 0.06\n  rhythm_strength: 0.90\n  harmonic_percussive_ratio: 3.36\n  mfcc1: -211.63\n  mfcc2: 166.03\n  mfcc3: -42.82\n  mfcc4: 33.65\n  mfcc5: -15.99\n\nCluster 1 - 64 anthems\nCountries: az.mp3, ge.mp3, gd.mp3, iq.mp3, gy.mp3, kr.mp3, gr.mp3, kz.mp3, bf.mp3, je.mp3, il.mp3, dz.mp3, ga.mp3, kn.mp3, do.mp3, dm.mp3, cd.mp3, bm.mp3, br.mp3, it.mp3, af.mp3, gn.mp3, ai.mp3, dj.mp3, cz.mp3, fo.mp3, gb.mp3, aw.mp3, gq.mp3, ad.mp3, ae.mp3, cf.mp3, ba.mp3, jp.mp3, eg.mp3, ma.mp3, km.mp3, im.mp3, ee.mp3, bi.mp3, jm.mp3, ax.mp3, gi.mp3, in.mp3, gl.mp3, fi.mp3, au.mp3, bg.mp3, eh.mp3, am.mp3, fj.mp3, de.mp3, al.mp3, is.mp3, ky.mp3, kh.mp3, ag.mp3, gt.mp3, ca.mp3, ec.mp3, gm.mp3, ar.mp3, dk.mp3, fm.mp3\nAverage characteristics:\n  tempo: [102.29716662]...\n  centroid_mean: 1451.12\n  rolloff_mean: 2679.39\n  zcr_mean: 0.04\n  rhythm_strength: 0.81\n  harmonic_percussive_ratio: 4.24\n  mfcc1: -269.26\n  mfcc2: 193.30\n  mfcc3: -27.07\n  mfcc4: 12.85\n  mfcc5: -0.55\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Function to visualize an anthem\ndef visualize_anthem(filepath, country_name):\n    y, sr = librosa.load(filepath, sr=None)\n    \n    plt.figure(figsize=(15, 10))\n    \n    # Plot waveform\n    plt.subplot(3, 1, 1)\n    librosa.display.waveshow(y, sr=sr)\n    plt.title(f\"{country_name} National Anthem - Waveform\")\n    \n    # Plot spectrogram\n    plt.subplot(3, 1, 2)\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title('Spectrogram')\n    \n    # Plot chromagram\n    plt.subplot(3, 1, 3)\n    chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n    librosa.display.specshow(chroma, y_axis='chroma', x_axis='time')\n    plt.colorbar()\n    plt.title('Chromagram')\n    \n    plt.tight_layout()\n    plt.savefig(f\"{country_name}_anthem_analysis.png\")\n    plt.close()\n\n# Visualize a few random anthems\nimport random\nsample_anthems = random.sample(list(df['country']), 5)\nfor country in sample_anthems:\n    filename = [f for f in os.listdir(anthems_dir) if f.startswith(country)][0]\n    filepath = os.path.join(anthems_dir, filename)\n    visualize_anthem(filepath, country)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T09:23:50.719409Z","iopub.execute_input":"2025-03-09T09:23:50.719795Z","iopub.status.idle":"2025-03-09T09:25:00.848328Z","shell.execute_reply.started":"2025-03-09T09:23:50.719766Z","shell.execute_reply":"2025-03-09T09:25:00.846960Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Function to analyze rhythm and melody patterns\ndef analyze_patterns(df, anthems_dir):\n    rhythm_patterns = {}\n    scale_patterns = {}\n    \n    for index, row in df.iterrows():\n        country = row['country']\n        filename = [f for f in os.listdir(anthems_dir) if f.startswith(country)][0]\n        filepath = os.path.join(anthems_dir, filename)\n        \n        # Load audio\n        y, sr = librosa.load(filepath, sr=None)\n        \n        # Rhythm pattern analysis\n        onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n        tempo, beats = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)\n        \n        if len(beats) >= 2:\n            # Calculate beat intervals\n            beat_intervals = np.diff(beats)\n            # Categorize rhythm pattern\n            if np.std(beat_intervals) < 0.1:\n                rhythm_pattern = \"Regular/March-like\"\n            elif np.mean(beat_intervals) > sr * 0.5:\n                rhythm_pattern = \"Slow/Stately\"\n            else:\n                rhythm_pattern = \"Variable\"\n        else:\n            rhythm_pattern = \"Undefined\"\n        \n        rhythm_patterns[country] = rhythm_pattern\n        \n        # Key and scale analysis\n        chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n        chroma_sum = np.sum(chroma, axis=1)\n        key = np.argmax(chroma_sum)\n        key_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n        \n        # Determine if major or minor based on relative presence of major and minor third\n        major_third = (key + 4) % 12\n        minor_third = (key + 3) % 12\n        if chroma_sum[major_third] > chroma_sum[minor_third]:\n            scale = \"Major\"\n        else:\n            scale = \"Minor\"\n            \n        scale_patterns[country] = f\"{key_names[key]} {scale}\"\n    \n    # Add to dataframe\n    df['rhythm_pattern'] = df['country'].map(rhythm_patterns)\n    df['key_and_scale'] = df['country'].map(scale_patterns)\n    \n    # Analysis of patterns\n    print(\"\\nRhythm Pattern Distribution:\")\n    print(df['rhythm_pattern'].value_counts())\n    \n    print(\"\\nKey and Scale Distribution:\")\n    print(df['key_and_scale'].value_counts().head(10))  # Top 10 most common keys\n    \n    return df\n\n# Enhance dataframe with pattern analysis\ndf = analyze_patterns(df, anthems_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T09:25:13.308820Z","iopub.execute_input":"2025-03-09T09:25:13.309258Z","iopub.status.idle":"2025-03-09T09:29:12.089078Z","shell.execute_reply.started":"2025-03-09T09:25:13.309227Z","shell.execute_reply":"2025-03-09T09:29:12.087290Z"}},"outputs":[{"name":"stdout","text":"\nRhythm Pattern Distribution:\nrhythm_pattern\nVariable    100\nName: count, dtype: int64\n\nKey and Scale Distribution:\nkey_and_scale\nF Major     27\nD# Major    17\nC Major     14\nA# Major     9\nD Minor      7\nA Minor      4\nG Major      3\nF Minor      3\nC Minor      3\nE Minor      3\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# You would need to add data about when each anthem was composed\n# This is just a conceptual example\n\n# Assuming you have added 'year_composed' to your dataframe\nif 'year_composed' in df.columns:\n    # Categorize by historical period\n    def get_period(year):\n        if year < 1800:\n            return \"Pre-1800\"\n        elif year < 1900:\n            return \"19th Century\"\n        elif year < 1950:\n            return \"Early 20th Century\"\n        elif year < 2000:\n            return \"Late 20th Century\"\n        else:\n            return \"21st Century\"\n            \n    df['historical_period'] = df['year_composed'].apply(get_period)\n    \n    # Analyze features by historical period\n    for feature in ['tempo', 'harmonic_percussive_ratio', 'duration']:\n        plt.figure(figsize=(10, 6))\n        df.boxplot(column=feature, by='historical_period')\n        plt.title(f'{feature} by Historical Period')\n        plt.suptitle('')\n        plt.savefig(f'{feature}_by_period.png')\n        plt.close()\n        \n    # Look for trends in key usage over time\n    period_key_counts = df.groupby(['historical_period', 'key_and_scale']).size().unstack(fill_value=0)\n    period_key_counts.plot(kind='bar', stacked=True, figsize=(12, 8))\n    plt.title('Key Usage by Historical Period')\n    plt.ylabel('Number of Anthems')\n    plt.tight_layout()\n    plt.savefig('key_usage_by_period.png')\n    plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T09:32:55.073522Z","iopub.execute_input":"2025-03-09T09:32:55.073931Z","iopub.status.idle":"2025-03-09T09:32:55.083107Z","shell.execute_reply.started":"2025-03-09T09:32:55.073902Z","shell.execute_reply":"2025-03-09T09:32:55.081404Z"}},"outputs":[],"execution_count":9}]}