{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting Pillow\n",
      "  Downloading pillow-11.1.0-cp313-cp313-win_amd64.whl.metadata (9.3 kB)\n",
      "Downloading pillow-11.1.0-cp313-cp313-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 1.0/2.6 MB 3.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.8/2.6 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 3.4 MB/s eta 0:00:00\n",
      "Installing collected packages: Pillow\n",
      "Successfully installed Pillow-11.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset collection for MultiModalVerse...\n",
      "Target: 20 categories with 50 images each\n",
      "\n",
      "Dataset collection complete!\n",
      "Total images collected: 1000\n",
      "\n",
      "Use Case for MultiModalVerse Dataset:\n",
      "1. Multi-class image classification model training\n",
      "2. Image search and retrieval systems\n",
      "3. Computer vision model evaluation across diverse categories\n",
      "4. Transfer learning experiments with varied data domains\n",
      "5. Data augmentation technique testing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='image_collection.log'\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Dataset name\n",
    "DATASET_NAME = \"Image_MultiModalVerse\"\n",
    "base_dir = f\"{DATASET_NAME}_dataset\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# List of 20 categories as specified in the task\n",
    "categories = [\n",
    "    \"cats\", \"dogs\", \"cars\", \"bicycles\", \"mountains\", \"beaches\", \"planes\", \"flowers\", \"buildings\", \"birds\",\n",
    "    \"computers\", \"books\", \"trees\", \"fruits\", \"shoes\", \"clocks\", \"guitars\", \"robots\", \"bridges\", \"cakes\"\n",
    "]\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "chrome_driver_path = r\"C:\\Users\\UJWAL\\Downloads\\chromedriver-win64 (1)\\chromedriver-win64\\chromedriver.exe\"\n",
    "\n",
    "# Create a Service object\n",
    "service = Service(chrome_driver_path)\n",
    "\n",
    "# Add options for better performance\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode for better performance\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")  # Avoid detection\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\")\n",
    "\n",
    "# Initialize the WebDriver with the service and options\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Create metadata CSV file with all required fields\n",
    "metadata_file = os.path.join(base_dir, f\"{DATASET_NAME}_metadata.csv\")\n",
    "with open(metadata_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['category', 'filename', 'url', 'width', 'height', 'file_size_kb'])\n",
    "\n",
    "# Function to scrape image URLs - improved version\n",
    "def fetch_image_urls(query, max_images=50, max_retries=3):\n",
    "    for retry in range(max_retries):\n",
    "        try:\n",
    "            # Use different search engines for diversity\n",
    "            if retry == 0:\n",
    "                search_url = f\"https://www.google.com/search?q={query}&source=lnms&tbm=isch\"\n",
    "            elif retry == 1:\n",
    "                search_url = f\"https://www.bing.com/images/search?q={query}\"\n",
    "            else:\n",
    "                search_url = f\"https://search.yahoo.com/search?p={query}&tbm=isch\"\n",
    "                \n",
    "            driver.get(search_url)\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Scroll down multiple times to load more images\n",
    "            for _ in range(5):\n",
    "                driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "                time.sleep(2)\n",
    "            \n",
    "            # Get page source\n",
    "            page_source = driver.page_source\n",
    "            \n",
    "            # Parse with BeautifulSoup\n",
    "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "            \n",
    "            # Different image selectors for different search engines\n",
    "            if retry == 0:  # Google\n",
    "                img_tags = soup.find_all(\"img\", {\"class\": \"rg_i\"})\n",
    "                if not img_tags:\n",
    "                    img_tags = soup.find_all(\"img\")\n",
    "            elif retry == 1:  # Bing\n",
    "                img_tags = soup.find_all(\"img\", {\"class\": \"mimg\"})\n",
    "                if not img_tags:\n",
    "                    img_tags = soup.find_all(\"img\")\n",
    "            else:  # Yahoo\n",
    "                img_tags = soup.find_all(\"img\")\n",
    "            \n",
    "            # Extract image URLs\n",
    "            img_urls = []\n",
    "            for img in img_tags:\n",
    "                if \"src\" in img.attrs:\n",
    "                    src = img[\"src\"]\n",
    "                    if src.startswith('http') and src not in img_urls:\n",
    "                        img_urls.append(src)\n",
    "                elif \"data-src\" in img.attrs:\n",
    "                    src = img[\"data-src\"]\n",
    "                    if src.startswith('http') and src not in img_urls:\n",
    "                        img_urls.append(src)\n",
    "            \n",
    "            logger.info(f\"Found {len(img_urls)} images for {query}\")\n",
    "            \n",
    "            # If we found enough images, return them\n",
    "            if len(img_urls) >= max_images:\n",
    "                return img_urls[:max_images]\n",
    "            \n",
    "            # If we didn't find enough, try clicking on thumbnails to get full-res URLs\n",
    "            try:\n",
    "                thumbnail_elements = driver.find_elements(By.CSS_SELECTOR, \"img.rg_i\")\n",
    "                for thumbnail in thumbnail_elements[:max_images]:\n",
    "                    try:\n",
    "                        thumbnail.click()\n",
    "                        time.sleep(1)\n",
    "                        expanded_images = driver.find_elements(By.CSS_SELECTOR, \"img.r48jcc\")\n",
    "                        if expanded_images:\n",
    "                            src = expanded_images[0].get_attribute('src')\n",
    "                            if src and src.startswith('http') and src not in img_urls:\n",
    "                                img_urls.append(src)\n",
    "                    except:\n",
    "                        continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            logger.info(f\"After thumbnail expansion: Found {len(img_urls)} images for {query}\")\n",
    "            return img_urls[:max_images]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during URL fetching for {query}, attempt {retry+1}: {str(e)}\")\n",
    "            \n",
    "    # If all retries failed, return empty list\n",
    "    logger.error(f\"Failed to fetch URLs for {query} after {max_retries} attempts\")\n",
    "    return []\n",
    "\n",
    "# Function to download and validate an image\n",
    "def download_and_validate_image(img_url, category, index, max_retries=3):\n",
    "    filename = f\"{category}_{index:03d}.jpg\"\n",
    "    category_dir = os.path.join(base_dir, category)\n",
    "    file_path = os.path.join(category_dir, filename)\n",
    "    \n",
    "    for retry in range(max_retries):\n",
    "        try:\n",
    "            # Set headers to mimic a browser\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "                \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "                \"Referer\": \"https://www.google.com/\"\n",
    "            }\n",
    "            \n",
    "            # Download the image\n",
    "            response = requests.get(img_url, headers=headers, timeout=10)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                logger.warning(f\"Failed to download {img_url}: Status code {response.status_code}\")\n",
    "                raise Exception(f\"HTTP error: {response.status_code}\")\n",
    "            \n",
    "            # Try to open the image to validate it\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            \n",
    "            # Save the image\n",
    "            img.save(file_path)\n",
    "            \n",
    "            # Get the dimensions and file size\n",
    "            width, height = img.size\n",
    "            file_size_kb = os.path.getsize(file_path) / 1024\n",
    "            \n",
    "            # Add to metadata CSV\n",
    "            with open(metadata_file, 'a', newline='', encoding='utf-8') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([category, filename, img_url, width, height, file_size_kb])\n",
    "            \n",
    "            logger.info(f\"Successfully downloaded {category}/{filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error downloading {img_url} (attempt {retry+1}): {str(e)}\")\n",
    "            if retry == max_retries - 1:\n",
    "                logger.error(f\"Failed to download {img_url} after {max_retries} attempts\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Function to download images for a category\n",
    "def download_images_for_category(category, max_images=50):\n",
    "    category_dir = os.path.join(base_dir, category)\n",
    "    os.makedirs(category_dir, exist_ok=True)\n",
    "    \n",
    "    logger.info(f\"Starting download for category: {category}\")\n",
    "    \n",
    "    # Get image URLs\n",
    "    img_urls = fetch_image_urls(category, max_images=max_images*2)  # Get extra URLs in case some fail\n",
    "    \n",
    "    # Track the number of successful downloads\n",
    "    successful_downloads = 0\n",
    "    \n",
    "    # Try to download each image\n",
    "    for i, img_url in enumerate(img_urls):\n",
    "        if successful_downloads >= max_images:\n",
    "            break\n",
    "            \n",
    "        if download_and_validate_image(img_url, category, successful_downloads):\n",
    "            successful_downloads += 1\n",
    "            \n",
    "        # Slight delay to avoid being blocked\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    logger.info(f\"Completed downloads for {category}: {successful_downloads}/{max_images} images\")\n",
    "    return successful_downloads\n",
    "\n",
    "# Function to generate dataset summary\n",
    "def generate_dataset_summary():\n",
    "    try:\n",
    "        # Read the metadata file\n",
    "        df = pd.read_csv(metadata_file)\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        category_counts = df['category'].value_counts()\n",
    "        avg_dimensions = df[['width', 'height']].mean()\n",
    "        avg_file_size = df['file_size_kb'].mean()\n",
    "        \n",
    "        summary = {\n",
    "            'dataset_name': DATASET_NAME,\n",
    "            'total_images': len(df),\n",
    "            'categories': len(category_counts),\n",
    "            'images_per_category': category_counts.to_dict(),\n",
    "            'avg_width': avg_dimensions['width'],\n",
    "            'avg_height': avg_dimensions['height'],\n",
    "            'avg_file_size_kb': avg_file_size\n",
    "        }\n",
    "        \n",
    "        # Save summary to file\n",
    "        summary_file = os.path.join(base_dir, f\"{DATASET_NAME}_summary.txt\")\n",
    "        with open(summary_file, 'w') as f:\n",
    "            for key, value in summary.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "        logger.info(f\"Dataset summary saved to {summary_file}\")\n",
    "        return summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating summary: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Main execution\n",
    "try:\n",
    "    print(f\"Starting dataset collection for {DATASET_NAME}...\")\n",
    "    print(f\"Target: {len(categories)} categories with 50 images each\")\n",
    "    \n",
    "    total_images = 0\n",
    "    \n",
    "    # Process each category\n",
    "    for category in categories:\n",
    "        num_images = download_images_for_category(category, max_images=50)\n",
    "        total_images += num_images\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = generate_dataset_summary()\n",
    "    \n",
    "    print(f\"\\nDataset collection complete!\")\n",
    "    print(f\"Total images collected: {total_images}\")\n",
    "    \n",
    "    # Use case demonstration\n",
    "    print(\"\\nUse Case for MultiModalVerse Dataset:\")\n",
    "    print(\"1. Multi-class image classification model training\")\n",
    "    print(\"2. Image search and retrieval systems\")\n",
    "    print(\"3. Computer vision model evaluation across diverse categories\")\n",
    "    print(\"4. Transfer learning experiments with varied data domains\")\n",
    "    print(\"5. Data augmentation technique testing\")\n",
    "    \n",
    "finally:\n",
    "    # Close the WebDriver\n",
    "    driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
