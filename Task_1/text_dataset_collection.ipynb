{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ujwal\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.8 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\UJWAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\UJWAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Configure Selenium WebDriver\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--log-level=3\")  # Reduce logs\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    \"Technology\": [\n",
    "        \"https://techcrunch.com\", \n",
    "        \"https://www.wired.com\", \n",
    "        \"https://arstechnica.com\"\n",
    "    ],\n",
    "    \"Health\": [\n",
    "        \"https://www.healthline.com\", \n",
    "        \"https://www.medicalnewstoday.com\", \n",
    "        \"https://www.webmd.com\"\n",
    "    ],\n",
    "    \"Finance\": [\n",
    "        \"https://www.investopedia.com\", \n",
    "        \"https://www.forbes.com/money\", \n",
    "        \"https://www.ft.com\"\n",
    "    ],\n",
    "    \"Sports\": [\n",
    "        \"https://www.espn.com\", \n",
    "        \"https://www.skysports.com\", \n",
    "        \"https://www.bbc.com/sport\"\n",
    "    ],\n",
    "    \"Science\": [\n",
    "        \"https://www.sciencenews.org\", \n",
    "        \"https://www.nature.com\", \n",
    "        \"https://www.scientificamerican.com\"\n",
    "    ],\n",
    "    \"Entertainment\": [\n",
    "        \"https://www.hollywoodreporter.com\", \n",
    "        \"https://www.variety.com\", \n",
    "        \"https://www.rollingstone.com\"\n",
    "    ],\n",
    "    \"Politics\": [\n",
    "        \"https://www.politico.com\", \n",
    "        \"https://www.npr.org/sections/politics\", \n",
    "        \"https://www.cnn.com/politics\"\n",
    "    ],\n",
    "    \"Education\": [\n",
    "        \"https://www.edweek.org\", \n",
    "        \"https://www.timeshighereducation.com\", \n",
    "        \"https://www.theguardian.com/education\"\n",
    "    ],\n",
    "    \"Business\": [\n",
    "        \"https://www.bloomberg.com\", \n",
    "        \"https://www.businessinsider.com\", \n",
    "        \"https://www.cnbc.com/business/\"\n",
    "    ],\n",
    "    \"Automobile\": [\n",
    "        \"https://www.motortrend.com\", \n",
    "        \"https://www.cars.com\", \n",
    "        \"https://www.autoblog.com\"\n",
    "    ],\n",
    "    \"Travel\": [\n",
    "        \"https://www.lonelyplanet.com\", \n",
    "        \"https://www.cntraveler.com\", \n",
    "        \"https://www.nationalgeographic.com/travel\"\n",
    "    ],\n",
    "    \"Food\": [\n",
    "        \"https://www.bonappetit.com\", \n",
    "        \"https://www.foodnetwork.com\", \n",
    "        \"https://www.seriouseats.com\"\n",
    "    ],\n",
    "    \"Fashion\": [\n",
    "        \"https://www.vogue.com\", \n",
    "        \"https://www.elle.com\", \n",
    "        \"https://www.gq.com\"\n",
    "    ],\n",
    "    \"Music\": [\n",
    "        \"https://www.billboard.com\", \n",
    "        \"https://www.nme.com\", \n",
    "        \"https://pitchfork.com\"\n",
    "    ],\n",
    "    \"Gaming\": [\n",
    "        \"https://www.ign.com\", \n",
    "        \"https://www.pcgamer.com\", \n",
    "        \"https://www.gamespot.com\"\n",
    "    ],\n",
    "    \"Weather\": [\n",
    "        \"https://weather.com\", \n",
    "        \"https://www.accuweather.com\", \n",
    "        \"https://www.noaa.gov\"\n",
    "    ],\n",
    "    \"Cryptocurrency\": [\n",
    "        \"https://cointelegraph.com\", \n",
    "        \"https://www.coindesk.com\", \n",
    "        \"https://decrypt.co\"\n",
    "    ],\n",
    "    \"History\": [\n",
    "        \"https://www.history.com\", \n",
    "        \"https://www.smithsonianmag.com/history\", \n",
    "        \"https://www.bbc.co.uk/history\"\n",
    "    ],\n",
    "    \"Real Estate\": [\n",
    "        \"https://www.zillow.com\", \n",
    "        \"https://www.realtor.com\", \n",
    "        \"https://www.redfin.com\"\n",
    "    ],\n",
    "    \"Space\": [\n",
    "        \"https://www.nasa.gov\", \n",
    "        \"https://www.spacex.com\", \n",
    "        \"https://www.space.com\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store text files\n",
    "output_dir = \"collected_texts\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def fetch_text_selenium(url):\n",
    "    \"\"\"Fetch text from a website using Selenium and BeautifulSoup.\"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Allow page to load\n",
    "\n",
    "        # Get page source and parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        text = \" \".join([p.get_text() for p in paragraphs])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://www.history.com/news\n",
      "Scraping: https://www.bbc.co.uk/programmes/genres/factual/history/player\n",
      "Scraping: https://www.nationalgeographic.com/history\n",
      "Scraping: https://www.realtor.com/news/\n",
      "Scraping: https://www.zillow.com/research/\n",
      "Scraping: https://www.forbes.com/real-estate/\n",
      "Scraping: https://www.nasa.gov/news\n",
      "Scraping: https://www.space.com/news\n",
      "Scraping: https://www.esa.int/Newsroom\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Crawl websites and extract text\n",
    "for category, websites in categories.items():\n",
    "    collected_text = \"\"\n",
    "\n",
    "    for site in websites:\n",
    "        print(f\"Scraping: {site}\")\n",
    "        collected_text += fetch_text_selenium(site) + \"\\n\"\n",
    "\n",
    "    # Save collected text to a file\n",
    "    file_path = os.path.join(output_dir, f\"{category}.txt\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(collected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: NLP Preprocessing\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove punctuation, stopwords, and tokenize.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.isalpha() and word not in stopwords.words(\"english\")]  # Remove stopwords\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\UJWAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\UJWAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Technology successfully\n",
      "Processed Health successfully\n",
      "Processed Finance successfully\n",
      "Processed Sports successfully\n",
      "Processed Science successfully\n",
      "Processed Entertainment successfully\n",
      "Processed Politics successfully\n",
      "Processed Education successfully\n",
      "Processed Business successfully\n",
      "Processed Automobile successfully\n",
      "Processed Travel successfully\n",
      "Processed Food successfully\n",
      "Processed Fashion successfully\n",
      "Processed Music successfully\n",
      "Processed Gaming successfully\n",
      "Processed Weather successfully\n",
      "Processed Cryptocurrency successfully\n",
      "Processed History successfully\n",
      "Processed Real Estate successfully\n",
      "Processed Space successfully\n",
      "Text collection and preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from selenium import webdriver\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 3: NLP Preprocessing\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove punctuation, stopwords, and tokenize.\"\"\"\n",
    "    try:\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
    "        \n",
    "        # Use word_tokenize directly without relying on punkt_tab\n",
    "        words = text.split()  # Simple space-based tokenization as fallback\n",
    "        \n",
    "        # Remove stopwords\n",
    "        words = [word for word in words if word.isalpha() and word not in stopwords.words(\"english\")]\n",
    "        return \" \".join(words)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in clean_text function: {str(e)}\")\n",
    "        return text  # Return original text if cleaning fails\n",
    "\n",
    "# Process all collected text files\n",
    "for category in categories:\n",
    "    file_path = os.path.join(output_dir, f\"{category}.txt\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_text = f.read()\n",
    "        \n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        \n",
    "        # Save cleaned text\n",
    "        cleaned_file_path = os.path.join(output_dir, f\"{category}_cleaned.txt\")\n",
    "        with open(cleaned_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(cleaned_text)\n",
    "        \n",
    "        print(f\"Processed {category} successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {category}: {str(e)}\")\n",
    "\n",
    "# Close Selenium WebDriver\n",
    "try:\n",
    "    driver.quit()\n",
    "except:\n",
    "    print(\"Note: WebDriver may have already been closed\")\n",
    "\n",
    "print(\"Text collection and preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description:The WebText-20 dataset contains text data collected from 20 diverse categories across multiple websites.\n",
      " The dataset is useful for text analysis, NLP tasks, sentiment analysis, topic modeling, and machine learning applications. \n",
      "Each category contains textual data extracted from three different websites\n"
     ]
    }
   ],
   "source": [
    "print(\"Description:The WebText-20 dataset contains text data collected from 20 diverse categories across multiple websites.\\n The dataset is useful for text analysis, NLP tasks, sentiment analysis, topic modeling, and machine learning applications. \\nEach category contains textual data extracted from three different websites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The usecases are as follows:\n",
      "Topic Modeling & Clustering\n",
      "Sentiment Analysis\n",
      "Text Generation & Chatbots\n",
      "Trend Analysis & Forecasting\n",
      "Fake News Detection\n"
     ]
    }
   ],
   "source": [
    "print(\"The usecases are as follows:\")\n",
    "print(\"Topic Modeling & Clustering\")\n",
    "print(\"Sentiment Analysis\")\n",
    "print(\"Text Generation & Chatbots\")\n",
    "print(\"Trend Analysis & Forecasting\")\n",
    "print(\"Fake News Detection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
